# LLM Model Configuration
# Defines model aliases and defaults for LM Studio and Ollama endpoints

# Default LLM endpoint (LM Studio)
default_endpoint: "https://lm.leophir.com/"

# Ollama endpoint (currently offline, for future use)
ollama_endpoint: "http://oll.leophir.com/"

# Model aliases - map friendly names to actual model IDs
models:
  # Primary models - dual configuration for performance vs quality
  small_model:
    id: "mistralai/mistral-7b-instruct-v0.3"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.4
    top_k: 30
    top_p: 0.9
    max_tokens: 4096
    description: "Mistral 7B - Fast extraction, SEO audit, categorization"

  large_model:
    id: "meta-llama-3-8b-instruct.gguf"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.7  # For creative writing
    top_k: 40
    top_p: 0.9
    max_tokens: 8192
    description: "Llama 3 8B - French writing, outreach, advanced reasoning"

  # Legacy models (preserved for compatibility)
  gpt-oss-20b:
    id: "openai/gpt-oss-20b"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.2
    max_tokens: 2048
    description: "GPT-OSS 20B - General purpose model"

  qwen-4b:
    id: "qwen/qwen3-4b-2507"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.3
    max_tokens: 2048
    description: "Qwen 4B - Fast, efficient model for quick tasks"

  qwen-7b:
    id: "qwen/qwen3-7b"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.2
    max_tokens: 4096
    description: "Qwen 7B - Balanced performance"

  # Task-specific presets
  classification:
    id: "mistralai/mistral-7b-instruct-v0.3"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.4
    top_k: 30
    top_p: 0.9
    max_tokens: 1024
    description: "Optimized for lead classification (uses small_model)"

  outreach:
    id: "meta-llama-3-8b-instruct.gguf"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.7  # Higher temp for creative writing
    top_k: 40
    top_p: 0.9
    max_tokens: 1024
    description: "Optimized for personalized outreach drafts (uses large_model)"

  dossier:
    id: "meta-llama-3-8b-instruct.gguf"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.6  # Balanced for analysis
    top_k: 40
    top_p: 0.9
    max_tokens: 3072
    description: "Optimized for dossier building (uses large_model)"

  audit:
    id: "mistralai/mistral-7b-instruct-v0.3"
    endpoint: "https://lm.leophir.com/"
    temperature: 0.4
    top_k: 30
    top_p: 0.9
    max_tokens: 2048
    description: "Optimized for SEO audits (uses small_model)"

# Ollama models (for future use when endpoint is online)
ollama_models:
  llama3:
    id: "llama3"
    endpoint: "http://oll.leophir.com/"
    temperature: 0.2
    max_tokens: 2048
    description: "Llama 3 via Ollama"

  mistral:
    id: "mistral"
    endpoint: "http://oll.leophir.com/"
    temperature: 0.2
    max_tokens: 2048
    description: "Mistral via Ollama"
