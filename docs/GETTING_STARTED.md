# Getting Started with the Streamlit Toolkit

Lead Hunter Toolkit now ships with a Streamlit dashboard that brings all the
research, enrichment, and delivery workflows into one place. The quickest way
to explore the app is to launch it locally and walk through the Search Scraper
tab, which bundles the legacy scrapegraph.js features together with the new
contact discovery pipeline and site indexer.

## Launch the application

1. Install the project dependencies (a virtual environment is recommended):
   ```bash
   pip install -r requirements.txt
   ```
2. Start the dashboard:
   ```bash
   streamlit run app.py
   ```
3. Streamlit will print a local URL (typically `http://localhost:8501`). Open
   it in your browser to access all tabs of the toolkit. The entry point wires
   the sidebar, settings persistence, and every tab from a single `main()`
   function so you do not need to write any glue code yourself.ã€F:app.pyâ€ L1-L88ã€‘

## Run web research with Search Scraper

Open the **Search Scraper** tab in the navigation bar once the app loads. The
panel exposes every control you need to go from a natural-language question to
structured answers or raw markdown copies of the pages that were fetched.

1. Provide your research question in the **Research Question or Query** text
   area and choose how many sources to fetch. The selector lets you switch
   between **AI Extraction** (LLM synthesis) and **Markdown** (concatenated page
   exports).ã€F:ui/tabs/search_scraper_tab.pyâ€ L33-L90ã€‘
2. (Optional) Tick **Use custom extraction schema (advanced)** to paste a JSON
   schema that guides the LLM output when you are in AI Extraction mode.ã€F:ui/tabs/search_scraper_tab.pyâ€ L92-L115ã€‘
3. Click **ğŸ” Search & Scrape**. Progress indicators will walk through search,
   fetching, conversion, and extraction steps while the underlying
   `SearchScraper` orchestrates the workflow.ã€F:ui/tabs/search_scraper_tab.pyâ€ L117-L156ã€‘ã€F:search_scraper.pyâ€ L15-L164ã€‘
4. Once the run finishes you can expand the **ğŸ“š Sources** section, review the
   synthesized insights or markdown payload, and export the results directly
   from the UI.ã€F:ui/tabs/search_scraper_tab.pyâ€ L158-L207ã€‘

## Use the contact discovery pipeline

Beneath the research results you will find the **Contact Discovery Pipeline**.
It can crawl a single website or execute a search query, consolidate all emails,
phone numbers, social profiles, andâ€”new in this releaseâ€”structured contact data
parsed from schema.org markup.

1. Pick **Website Crawl** or **Search Query** as the source mode.
2. Select which signals to capture by toggling **Extract Emails**, **Extract
   Phones**, **Extract Social**, and **Parse Structured Data**. The structured
   toggle enables schema.org JSON-LD and microdata parsing so contacts exposed
   in structured data snippets are merged alongside the traditional scraping
   output.ã€F:ui/tabs/search_scraper_tab.pyâ€ L209-L243ã€‘
3. Provide either the website URL or the search string, adjust crawl limits,
   and launch the pipeline. The Streamlit tab calls into the synchronous
   pipeline helpers so everything runs in one click.ã€F:ui/tabs/search_scraper_tab.pyâ€ L245-L338ã€‘ã€F:scraping/pipeline.pyâ€ L122-L219ã€‘
4. Review metrics, expand the contact lists, preview the processed markdown,
   and download the aggregated JSON payload once the run completes.ã€F:ui/tabs/search_scraper_tab.pyâ€ L340-L402ã€‘

## Enable and query the site indexer

Each Search Scraper run automatically syncs processed markdown into a lightweight
local vector index so you can revisit prior crawls without fetching the web
again. The index lives under `out/site_index` and is cached between sessions.

* The tab initialises a shared `SiteIndexer` instance backed by
  `embeddings.npy` and `metadata.json` files in that directory. The caption at
  the top of the Search Scraper panel shows how many chunks are currently
  available.ã€F:ui/tabs/search_scraper_tab.pyâ€ L37-L50ã€‘ã€F:indexing/site_indexer.pyâ€ L58-L91ã€‘
* During a scrape each page that successfully converts to markdown is chunked
  and added to the index together with metadata describing the prompt, mode,
  and source URL.ã€F:search_scraper.pyâ€ L130-L156ã€‘
* You can query the index from a notebook or script by instantiating
  `SiteIndexer("out/site_index")` and calling `.query("your question", top_k=5)`.
  The query method filters by domain or date when those parameters are supplied
  and returns rich metadata for each chunk so you can surface prior insights in
  custom workflows.ã€F:indexing/site_indexer.pyâ€ L24-L116ã€‘ã€F:indexing/site_indexer.pyâ€ L220-L283ã€‘

## Structured-data extraction controls

Structured-data parsing is enabled by default across the toolkit and can be
managed from two places:

* The global setting is stored in `config/defaults.yml` under
  `extraction.structured`, ensuring schema.org data is parsed even when you do
  not toggle the UI manually.ã€F:config/defaults.ymlâ€ L26-L34ã€‘
* The Search Scraper pipeline exposes a dedicated **Parse Structured Data**
  checkbox so you can turn structured extraction on or off for each run without
  touching configuration files.ã€F:ui/tabs/search_scraper_tab.pyâ€ L209-L243ã€‘

## Dynamic rendering with Playwright

Some modern websites require JavaScript execution before their content is
available. Lead Hunter Toolkit supports Playwright-backed fetching for those
cases.

1. Install the browser runtime once on your machine:
   ```bash
   pip install playwright
   playwright install chromium
   ```
2. Open the sidebar and switch to the **Search & Crawl** settings tab. This is
   the same panel that lets you fine-tune fetch timeouts, concurrency, and
   contact extraction defaults. In the enhanced sidebar build the
   **Dynamic rendering (Playwright)** toggle sits alongside these controlsâ€”flip
   it on to persist the preference to `settings.json`. If you are on an older
   layout, you can achieve the same effect by adding
   `"dynamic_rendering": true` to the saved settings file before launching the
   app.ã€F:sidebar_enhanced.pyâ€ L32-L116ã€‘ã€F:app.pyâ€ L8-L44ã€‘
3. With the toggle enabled, downstream helpers (such as `fetch_many` and
   `crawl_site`) receive `dynamic_rendering=True` and route eligible URLs
   through the Playwright workflow. Content is cached separately using a
   `dynamic::` cache key to avoid mixing rendered and static responses.ã€F:fetch.pyâ€ L50-L105ã€‘ã€F:crawl.pyâ€ L252-L303ã€‘

If you only need dynamic rendering for specific domains, add them to the
allowlist arguments provided by the lower-level helpers before running your
custom scripts. The caching layer will ensure that subsequent runs reuse the
rendered HTML when possible.ã€F:fetch.pyâ€ L56-L105ã€‘

With these steps you can run the Streamlit app end to end, perform AI-assisted
research, build contact lists, and revisit indexed findings without re-running
expensive crawls.
